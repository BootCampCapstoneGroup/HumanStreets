{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HumanStreets Segmentation - Colab Version (T4) - SAM3\n",
                "\n",
                "This notebook runs the segmentation pipeline using **SAM3** (Segment Anything Model 3 / Custom) and saves the results to a **GeoPackage (GPKG)** file with separate layers for each class.\n",
                "It uses the tiling logic from `load_segment_upload.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install rasterio ultralytics geopandas shapely pyproj tqdm git+https://github.com/ultralytics/CLIP.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5d0f0f25",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Set PyTorch memory configuration BEFORE importing torch\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
                "\n",
                "import rasterio\n",
                "import numpy as np\n",
                "import time\n",
                "import geopandas as gpd\n",
                "import pandas as pd\n",
                "import torch\n",
                "import gc\n",
                "from shapely.geometry import Polygon, MultiPolygon\n",
                "from rasterio.windows import Window\n",
                "from itertools import product\n",
                "from pyproj import Transformer\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Check for GPU\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "device = 0 if torch.cuda.is_available() else 'cpu'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5cccb04",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Mount Drive & Paths\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "print(\"Mounting Google Drive... Please check for an authentication popup!\")\n",
                "try:\n",
                "    drive.mount('/content/drive', force_remount=True)\n",
                "except Exception as e:\n",
                "    print(f\"Mount failed: {e}. Try running this cell again.\")\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "# Change this to your folder in Drive\n",
                "BASE_DIR = \"/content/drive/MyDrive/Spatial_Data_Bootcamp/captstone/Colab_Walkability\"\n",
                "\n",
                "TIF_PATH = os.path.join(BASE_DIR, \"mosiac_rgb_6cmPerPixel.tif\")\n",
                "MODEL_PATH = os.path.join(BASE_DIR, \"sam3.pt\")\n",
                "STREETS_PATH = os.path.join(BASE_DIR, \"streets.geojson\")\n",
                "\n",
                "OUTPUT_GPKG = os.path.join(BASE_DIR, \"sam3_results.gpkg\")\n",
                "\n",
                "print(f\"Checking files:\\n TIF: {os.path.exists(TIF_PATH)}\\n Model: {os.path.exists(MODEL_PATH)}\\n Streets: {os.path.exists(STREETS_PATH)}\")\n",
                "# TILE_SIZE updated to 1036 to match stride 14 requirement (1036 = 74 * 14)\n",
                "TILE_SIZE = 1036 \n",
                "OVERLAP = 50\n",
                "\n",
                "# Classes to detect with SAM3\n",
                "SAM3_CLASSES = [\"road\", \"sidewalk\", \"car\", \"obstacle\", \"tree\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da1d60e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import SAM Predictor\n",
                "# We try to import the specific predictor used in the project's other notebooks\n",
                "try:\n",
                "    from ultralytics.models.sam.predict import SAM3SemanticPredictor\n",
                "    print(\"Using SAM3SemanticPredictor\")\n",
                "except ImportError:\n",
                "    print(\"SAM3SemanticPredictor not found, falling back to standard SAM or checking imports...\")\n",
                "    # Fallback or error handling - assuming the environment will support it as per Run_SAM3_Colab.ipynb\n",
                "    from ultralytics import SAM\n",
                "    # Note: Standard SAM might not support 'text' prompts the same way.\n",
                "    # If this fails, ensure you are using the correct modified ultralytics version or script.\n",
                "\n",
                "def setup_transformer(src):\n",
                "    \"\"\"Setup coordinate transformer and bounds boundaries.\"\"\"\n",
                "    to_wgs84 = Transformer.from_crs(src.crs, \"EPSG:4326\", always_xy=True)\n",
                "    bounds = src.bounds\n",
                "    min_lon, min_lat = to_wgs84.transform(bounds.left, bounds.bottom)\n",
                "    max_lon, max_lat = to_wgs84.transform(bounds.right, bounds.top)\n",
                "    return to_wgs84, (min_lat, min_lon, max_lat, max_lon)\n",
                "\n",
                "def validate_and_correct_poly(poly):\n",
                "    \"\"\"Ensure polygon is valid, simple, and not empty.\"\"\"\n",
                "    if not poly.is_valid:\n",
                "        poly = poly.buffer(0)\n",
                "    if poly.is_empty:\n",
                "        return []\n",
                "    if isinstance(poly, MultiPolygon):\n",
                "        return [p for p in poly.geoms if p.is_valid and not p.is_empty]\n",
                "    return [poly]\n",
                "\n",
                "def process_image_sam3(tif_path, model_path, output_gpkg):\n",
                "    \"\"\"Run tiled inference using SAM3 and save to GPKG layers.\"\"\"\n",
                "    \n",
                "    if not os.path.exists(tif_path):\n",
                "        print(f\"Error: TIF file not found at {tif_path}\")\n",
                "        return\n",
                "\n",
                "    # Load Model\n",
                "    print(f\"Loading SAM3 model from {model_path}...\")\n",
                "    try:\n",
                "        # Initialize SAM3 Predictor\n",
                "        # overrides logic taken from Run_SAM3_Colab.ipynb\n",
                "        try:\n",
                "            predictor = SAM3SemanticPredictor(overrides=dict(conf=0.25, task=\"segment\", mode=\"predict\", model=model_path, imgsz=TILE_SIZE))\n",
                "        except NameError:\n",
                "            # Fallback if class not imported\n",
                "            predictor = SAM(model_path)\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Model Load Error: {e}\")\n",
                "        return\n",
                "\n",
                "    # Initialize storage for each class\n",
                "    class_polygons = {name: [] for name in SAM3_CLASSES}\n",
                "\n",
                "    with rasterio.open(tif_path) as src:\n",
                "        W, H = src.width, src.height\n",
                "        transform_affine = src.transform\n",
                "        transformer, wgs_bounds = setup_transformer(src)\n",
                "        min_lat, min_lon, max_lat, max_lon = wgs_bounds\n",
                "\n",
                "        print(f\"Processing Image: {W}x{H} | CRS: {src.crs}\")\n",
                "\n",
                "        # Tile Generation with Overlap\n",
                "        stride = TILE_SIZE - OVERLAP\n",
                "        # We use a simple range loop for stride.\n",
                "        # Ensure the last tile covers the edge by going up to W and H\n",
                "        x_anchors = list(range(0, W, stride))\n",
                "        y_anchors = list(range(0, H, stride))\n",
                "        \n",
                "        tiles = list(product(x_anchors, y_anchors))\n",
                "        print(f\"Total Tiles: {len(tiles)} (Size: {TILE_SIZE}, Overlap: {OVERLAP})\")\n",
                "\n",
                "        # Enumerate to track index for memory management\n",
                "        for i, (col, row) in enumerate(tqdm(tiles, desc=\"Processing Tiles\")):\n",
                "            \n",
                "            # Memory Cleanup every 10 tiles\n",
                "            if i % 10 == 0:\n",
                "                torch.cuda.empty_cache()\n",
                "                gc.collect()\n",
                "\n",
                "            # Read Tile\n",
                "            # Window(col, row, width, height)\n",
                "            # Rasterio handles truncation if col+width > W\n",
                "            window = Window(col, row, min(TILE_SIZE, W - col), min(TILE_SIZE, H - row))\n",
                "            img_data = src.read([1, 2, 3], window=window)\n",
                "            \n",
                "            # Check if tile has data\n",
                "            if img_data.shape[0] < 3 or img_data.max() == 0: continue\n",
                "            \n",
                "            # Check if tile is too small (might happen at very edge if stride aligns oddly, but usually fine)\n",
                "            if img_data.shape[1] < 10 or img_data.shape[2] < 10: continue\n",
                "\n",
                "            # Prepare Image (HCC -> HWC, Contiguous)\n",
                "            img = np.ascontiguousarray(np.transpose(img_data, (1, 2, 0)))\n",
                "            \n",
                "            # Inference - NO GRAD to save memory\n",
                "            with torch.no_grad():\n",
                "                # We assume predictor works like in Run_SAM3_Colab.ipynb\n",
                "                # set_image might be needed if using SemanticPredictor clss specifically\n",
                "                if hasattr(predictor, 'set_image'):\n",
                "                    predictor.set_image(img)\n",
                "                    results = predictor(text=SAM3_CLASSES, save=False, verbose=False)\n",
                "                else:\n",
                "                    # Fallback usage\n",
                "                    results = predictor(img, imgsz=TILE_SIZE, verbose=False)\n",
                "\n",
                "            if not results or results[0].masks is None: \n",
                "                del img, img_data, results\n",
                "                continue\n",
                "            \n",
                "            res = results[0]\n",
                "            names_map = res.names\n",
                "            \n",
                "            for j, poly_coords in enumerate(res.masks.xy):\n",
                "                if len(poly_coords) < 3: continue\n",
                "                \n",
                "                # Identify Class\n",
                "                class_id = int(res.boxes.cls[j])\n",
                "                \n",
                "                # FIXED: Handle names_map as list or dict\n",
                "                class_name_raw = \"unknown\"\n",
                "                if hasattr(names_map, 'get'):\n",
                "                     class_name_raw = names_map.get(class_id, \"unknown\")\n",
                "                elif isinstance(names_map, list):\n",
                "                     if 0 <= class_id < len(names_map):\n",
                "                          class_name_raw = names_map[class_id]\n",
                "\n",
                "                # 1. Tile -> Global Pixel\n",
                "                global_x = poly_coords[:, 0] + col\n",
                "                global_y = poly_coords[:, 1] + row\n",
                "                \n",
                "                # 2. Global Pixel -> Native CRS\n",
                "                native_x, native_y = rasterio.transform.xy(transform_affine, global_y, global_x)\n",
                "                \n",
                "                # 3. Native -> WGS84\n",
                "                wgs_lon, wgs_lat = transformer.transform(native_x, native_y)\n",
                "                \n",
                "                # Validation\n",
                "                if not (np.all(np.isfinite(wgs_lon)) and np.all(np.isfinite(wgs_lat))): continue\n",
                "                if np.any(np.abs(wgs_lat) > 90) or np.any(np.abs(wgs_lon) > 180): continue\n",
                "\n",
                "                # Create Polygon\n",
                "                raw_poly = Polygon(zip(wgs_lon, wgs_lat))\n",
                "                \n",
                "                # Validate geometry\n",
                "                valid_polys = validate_and_correct_poly(raw_poly)\n",
                "                \n",
                "                # Assign to correct class bucket\n",
                "                for p in valid_polys:\n",
                "                    matched = False\n",
                "                    # Match returned class name to our target list\n",
                "                    for target in SAM3_CLASSES:\n",
                "                        if target in class_name_raw.lower():\n",
                "                            class_polygons[target].append(p)\n",
                "                            matched = True\n",
                "                            break\n",
                "                    # If not matched to specific list, maybe skip or add to 'other' if needed\n",
                "            \n",
                "            # Clean up per loop to free memory\n",
                "            del img, img_data, results, res\n",
                "                        \n",
                "\n",
                "    # --- Save to GPKG ---\n",
                "    print(f\"Saving results to {output_gpkg}...\")\n",
                "    \n",
                "    save_count = 0\n",
                "    for cls_name, polys in class_polygons.items():\n",
                "        if len(polys) > 0:\n",
                "            print(f\"  - Layer '{cls_name}': {len(polys)} polygons\")\n",
                "            gdf = gpd.GeoDataFrame({'geometry': polys}, crs=\"EPSG:4326\")\n",
                "            # Use layer=cls_name to create separate layers\n",
                "            gdf.to_file(output_gpkg, layer=cls_name, driver=\"GPKG\")\n",
                "            save_count += 1\n",
                "    \n",
                "    if save_count > 0:\n",
                "        print(f\"Success! Saved {save_count} layers to {output_gpkg}\")\n",
                "    else:\n",
                "        print(\"No polygons found to save.\")\n",
                "\n",
                "def main():\n",
                "    print(\"--- Starting SAM3 Segmentation Pipeline ---\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    process_image_sam3(TIF_PATH, MODEL_PATH, OUTPUT_GPKG)\n",
                "    \n",
                "    print(f\"Total Time: {time.time() - start_time:.2f}s\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
